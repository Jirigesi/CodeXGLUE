{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients, LayerActivation\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb12e761b2764738b1801fb7bd23d205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffab839880a34c4092bc4bb2c77d6d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e4fb89c27d41fe96abc2ec9c35c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f0a90a5fe5460bab0994609937eae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65f9ec8274d47998ac753d1eb60ba29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f6f15ad0c9407386e5abce1123f421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_path = 'deepset/roberta-base-squad2'\n",
    "\n",
    "# load model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path, output_attentions=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits, output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs, position=0):\n",
    "    pred = model(inputs_embeds=inputs)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "ref_token_id, sep_token_id, cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "    \n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"What is important to us?\", \"It is important to us to include, empower and support humans of all kinds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = 'to include, empower and support humans of all kinds'\n",
    "\n",
    "ground_truth_tokens = tokenizer.encode(ground_truth, add_special_tokens=False)\n",
    "ground_truth_end_ind = indices.index(ground_truth_tokens[-1])\n",
    "ground_truth_start_ind = ground_truth_end_ind - len(ground_truth_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  2264,    16,   505,     7,   201,   116,     2,   243,    16,\n",
       "           505,     7,   201,     7,   680,     6, 15519,     8,   323,  5868,\n",
       "             9,    70,  6134,     4,     2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is important to us?\n",
      "Predicted Answer:  Ġto Ġinclude , Ġempower Ġand Ġsupport Ġhumans Ġof Ġall Ġkinds\n"
     ]
    }
   ],
   "source": [
    "start_scores, end_scores, output_attentions = predict(input_ids)\n",
    "                                #    token_type_ids=token_type_ids, \\\n",
    "                                #    position_ids=position_ids, \\\n",
    "                                #    attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / norm_fn(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InterpretableEmbeddingBase(\n",
       "  (embedding): Embedding(50265, 768, padding_idx=1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fjiriges/anaconda3/envs/cuBERT/lib/python3.8/site-packages/captum/attr/_models/base.py:188: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'roberta.embeddings.word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_start = []\n",
    "layer_attrs_end = []\n",
    "\n",
    "layer_attn_mat_start = []\n",
    "layer_attn_mat_end = []\n",
    "\n",
    "input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=token_type_ids, ref_token_type_ids=ref_token_type_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LayerConductance(squad_pos_forward_func, model.roberta.encoder.layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "                                  baselines=ref_input_embeddings, \n",
    "                                  additional_forward_args=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0023,  0.0008,  0.0015,  ...,  0.0003, -0.0010,  0.0003],\n",
       "          [ 0.0089, -0.0074,  0.0034,  ...,  0.0053, -0.0002,  0.0022],\n",
       "          [ 0.0001,  0.0010, -0.0055,  ...,  0.0068,  0.0019,  0.0001],\n",
       "          ...,\n",
       "          [ 0.0024, -0.0015,  0.0024,  ...,  0.0007,  0.0029,  0.0005],\n",
       "          [ 0.0027, -0.0005, -0.0029,  ..., -0.0008,  0.0009,  0.0006],\n",
       "          [-0.0004,  0.0002,  0.0022,  ..., -0.0005,  0.0011,  0.0010]]],\n",
       "        grad_fn=<SumBackward1>),\n",
       " tensor([[[[-1.2346e-04, -1.1475e-05, -3.8416e-05,  ...,  4.8486e-07,\n",
       "             1.7653e-06,  3.6897e-06],\n",
       "           [ 1.0477e-03,  6.1881e-02, -1.6728e-03,  ...,  5.2565e-06,\n",
       "             3.5546e-05,  1.7740e-04],\n",
       "           [-1.2809e-04,  3.0367e-03,  4.0127e-04,  ...,  3.0098e-05,\n",
       "             2.6549e-05,  1.9713e-05],\n",
       "           ...,\n",
       "           [-5.2128e-05,  2.3169e-04,  1.3344e-04,  ..., -1.9066e-04,\n",
       "             2.7491e-04,  2.9395e-04],\n",
       "           [-5.7210e-07, -8.0422e-05,  8.3225e-05,  ..., -3.6489e-03,\n",
       "            -1.9196e-03, -1.6401e-03],\n",
       "           [-1.9342e-04,  1.6582e-04,  8.8146e-05,  ...,  6.0945e-03,\n",
       "             8.5407e-03,  3.4812e-03]],\n",
       " \n",
       "          [[-1.0212e-03,  2.0173e-04, -6.9499e-05,  ...,  1.2819e-05,\n",
       "            -2.8951e-03,  3.8262e-05],\n",
       "           [ 2.0639e-04,  1.6243e-03,  1.0545e-03,  ...,  1.9808e-04,\n",
       "            -1.6564e-04,  8.2536e-04],\n",
       "           [ 3.4335e-05,  4.3115e-04,  1.7679e-03,  ..., -4.0985e-05,\n",
       "            -1.0258e-05,  1.1316e-04],\n",
       "           ...,\n",
       "           [-5.2816e-07,  8.1180e-04, -3.7688e-06,  ..., -6.8654e-04,\n",
       "            -3.6722e-04, -4.1748e-04],\n",
       "           [ 3.0258e-04, -6.4773e-05,  1.5803e-04,  ...,  7.0022e-04,\n",
       "            -1.5936e-03, -3.0335e-03],\n",
       "           [-2.2140e-04, -1.5572e-03, -2.1537e-04,  ...,  7.2098e-04,\n",
       "            -5.9158e-05,  6.0067e-03]],\n",
       " \n",
       "          [[-1.0568e-04, -3.6666e-04, -8.8446e-06,  ...,  7.2208e-04,\n",
       "            -3.0448e-05, -1.1171e-04],\n",
       "           [-1.5559e-03, -2.9522e-04,  1.3846e-03,  ..., -1.6257e-03,\n",
       "            -9.2985e-04, -3.3902e-03],\n",
       "           [-2.5270e-03,  1.0307e-03,  7.6351e-04,  ...,  5.2383e-04,\n",
       "            -6.5760e-04,  1.3813e-03],\n",
       "           ...,\n",
       "           [-1.5080e-03,  6.8098e-04,  6.7705e-04,  ...,  5.9133e-04,\n",
       "            -1.9731e-04, -1.2686e-04],\n",
       "           [-1.4819e-03, -1.7464e-03, -1.5928e-04,  ..., -2.6569e-04,\n",
       "             7.4679e-04, -2.3964e-04],\n",
       "           [ 4.0228e-03,  1.9205e-03, -2.6711e-04,  ..., -2.0117e-04,\n",
       "            -6.8468e-04,  3.5670e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.4197e-05, -2.5674e-04,  1.2158e-03,  ..., -2.3957e-05,\n",
       "             3.4852e-05, -1.7106e-04],\n",
       "           [-3.8285e-08, -1.2983e-05, -5.1785e-05,  ..., -1.6416e-07,\n",
       "             1.4194e-12, -2.9260e-13],\n",
       "           [ 2.2677e-09,  4.2895e-07, -4.7783e-04,  ...,  4.1936e-07,\n",
       "             1.3543e-07,  5.9314e-11],\n",
       "           ...,\n",
       "           [-6.5401e-10,  9.0332e-12, -2.4821e-09,  ..., -2.7246e-03,\n",
       "             1.7140e-02, -1.1746e-04],\n",
       "           [-4.0325e-10, -4.5530e-14, -9.1875e-14,  ..., -2.1956e-05,\n",
       "             1.0453e-02,  3.0783e-02],\n",
       "           [-7.9328e-08, -4.4370e-13, -1.5039e-13,  ..., -4.1515e-07,\n",
       "             1.5458e-02,  5.9147e-03]],\n",
       " \n",
       "          [[-6.8930e-05, -6.1873e-04,  4.1542e-04,  ...,  4.0555e-04,\n",
       "             2.6903e-04,  5.9816e-04],\n",
       "           [ 2.7748e-03, -3.1674e-03, -3.2542e-03,  ...,  6.5178e-04,\n",
       "             4.9850e-05, -5.5617e-04],\n",
       "           [-1.2637e-03, -6.4956e-05,  5.4904e-04,  ..., -6.7555e-04,\n",
       "             1.7077e-04, -3.4118e-04],\n",
       "           ...,\n",
       "           [-2.1322e-04,  1.4584e-03, -2.5156e-04,  ...,  3.1391e-03,\n",
       "             4.4659e-04,  1.9477e-04],\n",
       "           [-4.2448e-04,  3.5144e-04, -3.6980e-05,  ...,  7.6420e-04,\n",
       "            -3.5758e-04,  2.0170e-04],\n",
       "           [ 1.6051e-03, -6.4809e-04, -3.2312e-04,  ...,  1.7203e-03,\n",
       "             1.3407e-03,  1.1715e-03]],\n",
       " \n",
       "          [[-1.8902e-09,  1.4348e-04, -4.8643e-04,  ...,  2.2991e-04,\n",
       "             1.5755e-05, -1.9921e-06],\n",
       "           [ 2.7202e-02, -8.1691e-03, -5.6293e-06,  ...,  8.5929e-22,\n",
       "             2.6069e-27,  1.1559e-28],\n",
       "           [-2.0940e-11, -3.8193e-03,  5.7353e-03,  ..., -1.9509e-15,\n",
       "            -2.2406e-24, -2.9086e-29],\n",
       "           ...,\n",
       "           [ 1.0546e-15,  3.4552e-13, -1.1634e-09,  ...,  1.2657e-05,\n",
       "            -1.4010e-09, -3.0389e-16],\n",
       "           [ 2.4660e-16,  4.7293e-21,  1.1846e-13,  ..., -3.2542e-05,\n",
       "             3.0803e-05,  2.7737e-10],\n",
       "           [-4.4049e-15, -6.8632e-25, -1.1137e-19,  ..., -1.1721e-06,\n",
       "            -3.1622e-05, -5.6060e-05]]]], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuBERT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8492d8711f92acbd6f1f5de70669e0e0dfa9ae8288673c2cd77f66771fab39c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
