{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients, LayerActivation\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_path = 'deepset/roberta-base-squad2'\n",
    "\n",
    "# load model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path, output_attentions=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits, output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs, position=0):\n",
    "    pred = model(inputs_embeds=inputs)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "ref_token_id, sep_token_id, cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "    \n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"What is important to us?\", \"It is important to us to include, empower and support humans of all kinds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = 'to include, empower and support humans of all kinds'\n",
    "\n",
    "ground_truth_tokens = tokenizer.encode(ground_truth, add_special_tokens=False)\n",
    "ground_truth_end_ind = indices.index(ground_truth_tokens[-1])\n",
    "ground_truth_start_ind = ground_truth_end_ind - len(ground_truth_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  2264,    16,   505,     7,   201,   116,     2,   243,    16,\n",
       "           505,     7,   201,     7,   680,     6, 15519,     8,   323,  5868,\n",
       "             9,    70,  6134,     4,     2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is important to us?\n",
      "Predicted Answer:  Ġto Ġinclude , Ġempower Ġand Ġsupport Ġhumans Ġof Ġall Ġkinds\n"
     ]
    }
   ],
   "source": [
    "start_scores, end_scores, output_attentions = predict(input_ids)\n",
    "                                #    token_type_ids=token_type_ids, \\\n",
    "                                #    position_ids=position_ids, \\\n",
    "                                #    attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.__version__ >= '1.7.0':\n",
    "    norm_fn = torch.linalg.norm\n",
    "else:\n",
    "    norm_fn = torch.norm\n",
    "    \n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / norm_fn(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fjiriges/anaconda3/envs/cuBERT/lib/python3.8/site-packages/captum/attr/_models/base.py:188: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'roberta.embeddings.word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_start = []\n",
    "layer_attrs_end = []\n",
    "\n",
    "layer_attn_mat_start = []\n",
    "layer_attn_mat_end = []\n",
    "\n",
    "input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=token_type_ids, ref_token_type_ids=ref_token_type_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    lc = LayerConductance(squad_pos_forward_func, \n",
    "                          model.roberta.encoder.layer[i])\n",
    "    layer_attributions_start = lc.attribute(inputs=input_embeddings, \n",
    "                                            baselines=ref_input_embeddings, \n",
    "                                            additional_forward_args=())\n",
    "    layer_attributions_end = lc.attribute(inputs=input_embeddings, \n",
    "                                          baselines=ref_input_embeddings, \n",
    "                                          additional_forward_args=())\n",
    "    \n",
    "    layer_attrs_start.append(summarize_attributions(layer_attributions_start[0]))\n",
    "    layer_attrs_end.append(summarize_attributions(layer_attributions_end[0]))\n",
    "\n",
    "    layer_attn_mat_start.append(layer_attributions_start[1])\n",
    "    layer_attn_mat_end.append(layer_attributions_end[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 25, 768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer_attributions_start), len(layer_attributions_start[0]),len(layer_attributions_start[0][0]), len(layer_attributions_start[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = layer_attributions_start[0].sum(dim=-1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.3100,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.6354,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000], grad_fn=<SqueezeBackward1>),\n",
       " 25)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions, len(attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = attributions / norm_fn(attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6252,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7805,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0256, -0.5729,  0.0341,  0.0408, -0.1145, -0.0303, -0.3497, -0.0515,\n",
       "         -0.0216,  0.1048,  0.1301,  0.1270,  0.0524,  0.1302,  0.1871,  0.3055,\n",
       "          0.2256,  0.3622, -0.0029,  0.2535, -0.0385, -0.0221,  0.2685,  0.1251,\n",
       "          0.0243], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0318, -0.5343, -0.0524, -0.0074, -0.1024, -0.0116, -0.3164, -0.0766,\n",
       "          0.0246,  0.0976,  0.1128,  0.1142, -0.0031,  0.2055,  0.3382,  0.2990,\n",
       "          0.1909,  0.3454, -0.0092,  0.2624, -0.0121, -0.0252,  0.2887, -0.0026,\n",
       "          0.0574], grad_fn=<DivBackward0>),\n",
       " tensor([-0.0284, -0.3818, -0.0947, -0.0760, -0.1365, -0.1028, -0.4160,  0.1624,\n",
       "         -0.0346,  0.1017,  0.2810,  0.3585,  0.0787,  0.1321,  0.2101,  0.3839,\n",
       "          0.1219,  0.2913, -0.0268,  0.2041,  0.0560, -0.0177,  0.1124, -0.1031,\n",
       "          0.0842], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0109, -0.4184, -0.1975, -0.0038, -0.0119, -0.0513, -0.3020,  0.1148,\n",
       "         -0.0791,  0.1149,  0.4825,  0.3796, -0.0782,  0.1884,  0.2456,  0.3181,\n",
       "          0.1756,  0.1304,  0.0884,  0.0033,  0.0595,  0.0126,  0.0203,  0.0584,\n",
       "         -0.1266], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0189, -0.3447, -0.2759,  0.2648, -0.0423,  0.0555, -0.2042,  0.0027,\n",
       "         -0.0294,  0.1343,  0.3976,  0.2759, -0.1193,  0.3426,  0.4529,  0.1420,\n",
       "          0.1231,  0.0865,  0.0867,  0.0788, -0.0484,  0.0093,  0.0320, -0.0135,\n",
       "         -0.1949], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0409, -0.1366, -0.1922, -0.2040, -0.0273, -0.0059, -0.1394,  0.0380,\n",
       "          0.0776, -0.0048, -0.1590,  0.1949, -0.0676,  0.7352,  0.4444,  0.1040,\n",
       "          0.0719,  0.0394,  0.0283,  0.1531, -0.0726, -0.1060, -0.0234, -0.0024,\n",
       "         -0.0938], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0498,  0.0196, -0.1556, -0.1431, -0.0555, -0.1083, -0.0644,  0.0649,\n",
       "         -0.0897, -0.0555, -0.1983, -0.0912, -0.0048,  0.8317,  0.3708,  0.0939,\n",
       "          0.0707,  0.0233, -0.0045,  0.1492, -0.0142, -0.0533,  0.0328, -0.0155,\n",
       "         -0.0172], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0462, -0.0819, -0.0850, -0.0373, -0.0501, -0.1208, -0.1353, -0.1786,\n",
       "         -0.0753, -0.0880, -0.0597, -0.0324, -0.0093,  0.9031,  0.1671,  0.0243,\n",
       "          0.0271,  0.0199,  0.0719,  0.0442, -0.0025, -0.0120,  0.0288,  0.0318,\n",
       "         -0.2067], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0549, -0.0774, -0.0555, -0.0496, -0.0566, -0.1606, -0.0743,  0.0232,\n",
       "         -0.0231, -0.0562, -0.0104, -0.0375, -0.0446,  0.7547,  0.2173,  0.0151,\n",
       "          0.0258,  0.0302,  0.0826,  0.0822,  0.0081, -0.0027,  0.0119,  0.0199,\n",
       "         -0.5571], grad_fn=<DivBackward0>),\n",
       " tensor([ 0.0299, -0.0596, -0.0254, -0.0355, -0.0306, -0.2121, -0.0196, -0.0469,\n",
       "         -0.0128, -0.0279, -0.0602, -0.0206, -0.0640,  0.8286, -0.0064,  0.0173,\n",
       "          0.0137,  0.0010,  0.0217, -0.0076, -0.0116, -0.0433, -0.0461,  0.0100,\n",
       "         -0.4942], grad_fn=<DivBackward0>),\n",
       " tensor([-2.9126e-01, -1.9771e-02, -1.3355e-02, -1.8229e-02, -9.4693e-03,\n",
       "         -8.2962e-03, -5.7869e-03, -6.4632e-01,  1.5070e-02, -6.2172e-03,\n",
       "         -7.3250e-03, -1.7528e-03,  5.7349e-03,  5.5001e-01, -1.5328e-02,\n",
       "          9.2441e-03,  3.0045e-03,  2.1680e-03,  9.7234e-03,  3.8331e-02,\n",
       "         -4.7975e-04, -5.5589e-03, -7.8146e-03, -1.7149e-02, -4.3725e-01],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([-0.6252,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7805,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000], grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_attrs_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LayerConductance(squad_pos_forward_func, model.roberta.encoder.layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "                                  baselines=ref_input_embeddings, \n",
    "                                  additional_forward_args=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_start.append(summarize_attributions(layer_attributions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0256, -0.5729,  0.0341,  0.0408, -0.1145, -0.0303, -0.3497, -0.0515,\n",
       "         -0.0216,  0.1048,  0.1301,  0.1270,  0.0524,  0.1302,  0.1871,  0.3055,\n",
       "          0.2256,  0.3622, -0.0029,  0.2535, -0.0385, -0.0221,  0.2685,  0.1251,\n",
       "          0.0243], grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_attrs_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/fjiriges/CodeXGLUE/Code-Code/Defect-detection/code/demo_roberta.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpyxis.ics.uci.edu/home/fjiriges/CodeXGLUE/Code-Code/Defect-detection/code/demo_roberta.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m layer_attn_mat_start\u001b[39m.\u001b[39mappend(layer_attrs_start[\u001b[39m1\u001b[39;49m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "layer_attn_mat_start.append(layer_attrs_start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuBERT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8492d8711f92acbd6f1f5de70669e0e0dfa9ae8288673c2cd77f66771fab39c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
