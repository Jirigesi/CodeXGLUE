{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import javalang\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients, LayerActivation\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "if torch.__version__ >= '1.7.0':\n",
    "    norm_fn = torch.linalg.norm\n",
    "else:\n",
    "    norm_fn = torch.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer)}\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['roberta']\n",
    "config = config_class.from_pretrained('roberta-base')\n",
    "tokenizer = tokenizer_class.from_pretrained('roberta-base')\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm', \n",
    "                                           output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=ref_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cloze_words(filename, tokenizer):\n",
    "    with open(filename, 'r', encoding='utf-8') as fp:\n",
    "        words = fp.read().split('\\n')\n",
    "    idx2word = {tokenizer.encoder[w]: w for w in words}\n",
    "    return idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40492"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloze_results = []\n",
    "cloze_words_file = 'data/cloze-all/cloze_test_words.txt'\n",
    "file_path = 'data/cloze-all/java/clozeTest.json'\n",
    "\n",
    "idx2word = get_cloze_words(cloze_words_file, tokenizer)\n",
    "lines = json.load(open(file_path))\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40492\n"
     ]
    }
   ],
   "source": [
    "def read_answers(filename):\n",
    "    answers = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            answers[line.split('<CODESPLIT>')[0]] = line.split('<CODESPLIT>')[1]\n",
    "    return answers\n",
    "\n",
    "answer_file = 'evaluator/answers/java/answers.txt'\n",
    "answers = read_answers(answer_file)\n",
    "answer_list = list(answers.values())\n",
    "print(len(answer_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestSampleWithMaxPairLength = []\n",
    "bestSampleWithMaxPairLength_LEN =[]\n",
    "\n",
    "number_of_samples = 10\n",
    "for i in range(len(lines[:number_of_samples])):\n",
    "    code = ' '.join(lines[i]['pl_tokens'])\n",
    "    bestStr = \"<s> \" + code + \" </s>\"\n",
    "    bestLen = len(bestStr.split(\" \"))\n",
    "    bestSampleWithMaxPairLength.append(bestStr)\n",
    "    bestSampleWithMaxPairLength_LEN.append(bestLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths=[]\n",
    "codes=[]\n",
    "selected_answers = []\n",
    "\n",
    "for index, code in enumerate(bestSampleWithMaxPairLength):\n",
    "  l = len(tokenizer.tokenize(code))\n",
    "  if l<=256:\n",
    "    lengths.append(l)\n",
    "    codes.append(code)\n",
    "    selected_answers.append(answer_list[index])\n",
    "\n",
    "len(codes), len(selected_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract attribution score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / norm_fn(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forward_func(input_embeddings):\n",
    "    output = model(inputs_embeds=input_embeddings)\n",
    "    index = tokenized_text.index(tokenizer.mask_token_id)\n",
    "    if index > output.logits.shape[1]:\n",
    "        print(\"Length of outpu is {} and index is {}\".format(output.logits.shape[1], index))\n",
    "    output_list = output.logits[0][index]\n",
    "    output_list = output_list.unsqueeze(0)\n",
    "    \n",
    "    return output_list.max(1).values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use code as example\n",
    "# code = codes[0]\n",
    "# tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(code))\n",
    "# input_ids = torch.tensor([tokenized_text])\n",
    "# reference_indices = token_reference.generate_reference(input_ids.shape[1], device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fjiriges/anaconda3/envs/cuBERT/lib/python3.8/site-packages/captum/attr/_models/base.py:188: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'roberta.embeddings.word_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Average attribution on CLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:56<00:00,  9.35s/it]\n"
     ]
    }
   ],
   "source": [
    "cls_data = np.zeros((12,12))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for code in tqdm(codes):\n",
    "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(code))\n",
    "        input_ids = torch.tensor([tokenized_text])\n",
    "        reference_indices = token_reference.generate_reference(input_ids.shape[1], device=device).unsqueeze(0)\n",
    "\n",
    "        layer_attrs = []\n",
    "        layer_attn_mat = []\n",
    "        input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, reference_indices)\n",
    "\n",
    "        for i in range(model.config.num_hidden_layers):\n",
    "            lc = LayerConductance(predict_forward_func, \n",
    "                                model.roberta.encoder.layer[i])\n",
    "            layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "                                                    baselines=ref_input_embeddings, \n",
    "                                                    additional_forward_args=())\n",
    "            layer_attrs.append(summarize_attributions(layer_attributions[0]))\n",
    "            layer_attn_mat.append(layer_attributions[1])\n",
    "        # layer x seq_len\n",
    "        layer_attrs = torch.stack(layer_attrs)\n",
    "        # layer x batch x head x seq_len x seq_len\n",
    "        layer_attn_mat = torch.stack(layer_attn_mat)\n",
    "        for layer in range(12):\n",
    "            for head in range(12):\n",
    "                cls_data[layer][head] += layer_attn_mat[layer][0][head][:, 0:1].mean().cpu().detach().numpy()\n",
    "            \n",
    "CLS_atten = cls_data/len(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# indices = input_ids[0].detach().tolist()\n",
    "# all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# xticklabels=all_tokens\n",
    "# yticklabels=list(range(1,13))\n",
    "# ax = sns.heatmap(layer_attrs.cpu().detach().numpy(), xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2)\n",
    "# plt.xlabel('Tokens')\n",
    "# plt.ylabel('Layers')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12)\n",
      "[ 5.07964547e-06  1.50252481e-05  1.67530993e-05  6.84722125e-06\n",
      " -9.20095658e-07 -7.18242149e-06  1.39798530e-06 -9.33286918e-07\n",
      "  5.08806821e-06 -1.88176778e-06  8.03632843e-06  1.03664609e-05]\n"
     ]
    }
   ],
   "source": [
    "print(CLS_atten.shape)\n",
    "CLS_atten_sum = np.sum(CLS_atten, axis=1)\n",
    "print(CLS_atten_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Average attribution put on SEP token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:43<00:00,  7.31s/it]\n"
     ]
    }
   ],
   "source": [
    "sep_data = np.zeros((12,12))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for code in tqdm(codes):\n",
    "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(code))\n",
    "        input_ids = torch.tensor([tokenized_text])\n",
    "        reference_indices = token_reference.generate_reference(input_ids.shape[1], device=device).unsqueeze(0)\n",
    "\n",
    "        layer_attrs = []\n",
    "        layer_attn_mat = []\n",
    "        input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, reference_indices)\n",
    "\n",
    "        for i in range(model.config.num_hidden_layers):\n",
    "            lc = LayerConductance(predict_forward_func, \n",
    "                                model.roberta.encoder.layer[i])\n",
    "            layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "                                                    baselines=ref_input_embeddings, \n",
    "                                                    additional_forward_args=())\n",
    "            layer_attrs.append(summarize_attributions(layer_attributions[0]))\n",
    "            layer_attn_mat.append(layer_attributions[1])\n",
    "        # layer x seq_len\n",
    "        layer_attrs = torch.stack(layer_attrs)\n",
    "        # layer x batch x head x seq_len x seq_len\n",
    "        layer_attn_mat = torch.stack(layer_attn_mat)\n",
    "        for layer in range(12):\n",
    "          for head in range(12):\n",
    "            for each_sep_index in torch.where(input_ids[0]==2)[0].cpu().detach().numpy():\n",
    "              sep_data[layer][head] += layer_attn_mat[layer][0][head][:, each_sep_index].mean().cpu().detach().numpy() / len(torch.where(input_ids[0]==2)[0].cpu().detach().numpy())\n",
    "\n",
    "            \n",
    "SEP_atten = sep_data/len(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12)\n",
      "[ 1.22671537e-06 -1.32176880e-07 -4.75528458e-06 -2.41436848e-06\n",
      " -9.74803068e-07  3.97072681e-07 -8.00060733e-07 -1.49579204e-07\n",
      "  4.01521407e-07 -1.09889008e-06 -3.30790746e-08 -2.69541633e-06]\n"
     ]
    }
   ],
   "source": [
    "print(SEP_atten.shape)\n",
    "SEP_atten_sum = np.sum(SEP_atten, axis=1)\n",
    "print(SEP_atten_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average attention on Syntactic Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syntax_types_for_code(code_snippet):\n",
    "  types = [\"[CLS]\"]\n",
    "  code = [\"<s>\"]\n",
    "  tree = list(javalang.tokenizer.tokenize(code_snippet))\n",
    "  \n",
    "  for i in tree:\n",
    "    j = str(i)\n",
    "    j = j.split(\" \")\n",
    "    if j[1] == '\"MASK\"':\n",
    "      types.append('[MASK]')\n",
    "      code.append('<mask>')\n",
    "    else:\n",
    "      types.append(j[0].lower())\n",
    "      code.append(j[1][1:-1])\n",
    "    \n",
    "  types.append(\"[SEP]\")\n",
    "  code.append(\"</s>\")\n",
    "  return np.array(types), ' '.join(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_of_token_when_tokenized(code, types, tokenizer):\n",
    "  reindexed_types = []\n",
    "  start = 0\n",
    "  end = 0\n",
    "  for index, each_token in enumerate(code.split(\" \")):\n",
    "    tokenized_list = tokenizer.tokenize(each_token)\n",
    "    for i in range(len(tokenized_list)):\n",
    "      end += 1\n",
    "    reindexed_types.append((start, end-1))\n",
    "    start = end\n",
    "  return reindexed_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code = codes[3]\n",
    "\n",
    "# cleancode = code.replace(\"<s> \", \"\").replace(\" </s>\", \"\").replace('<mask>', 'MASK')\n",
    "# types, rewrote_code = get_syntax_types_for_code(cleancode)\n",
    "# len(types), len(rewrote_code.split(\" \"))\n",
    "# tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(rewrote_code))\n",
    "# input_ids = torch.tensor([tokenized_text])\n",
    "# reference_indices = token_reference.generate_reference(input_ids.shape[1], device=device).unsqueeze(0)\n",
    "\n",
    "# layer_attrs = []\n",
    "# layer_attn_mat = []\n",
    "# input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, reference_indices)\n",
    "\n",
    "# for i in range(model.config.num_hidden_layers):\n",
    "#     lc = LayerConductance(predict_forward_func, \n",
    "#                         model.roberta.encoder.layer[i])\n",
    "#     layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "#                                         baselines=ref_input_embeddings, \n",
    "#                                         additional_forward_args=())\n",
    "#     layer_attrs.append(summarize_attributions(layer_attributions[0]))\n",
    "#     layer_attn_mat.append(layer_attributions[1])\n",
    "\n",
    " # layer x seq_len\n",
    "# layer_attrs = torch.stack(layer_attrs)\n",
    "# # layer x batch x head x seq_len x seq_len\n",
    "# layer_attn_mat = torch.stack(layer_attn_mat)\n",
    "\n",
    "# # get start and end index of each token\n",
    "# start_end = get_start_end_of_token_when_tokenized(rewrote_code, types, tokenizer)\n",
    "# syntaxType = 'annotation'\n",
    "# for layer in range(12):\n",
    "#     for head in range(12):\n",
    "#         for each_sep_index in np.where(types==syntaxType)[0]:\n",
    "#             start_index, end_index = start_end[each_sep_index]\n",
    "#             interim_value = layer_attn_mat[layer][0][head][:, start_index:end_index+1].mean().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSyntaxAttributionScore(codes, tokenizer, syntaxType):\n",
    "\n",
    "  with torch.no_grad():\n",
    "    identifier = np.zeros((12,12))\n",
    "    number = 0 \n",
    "    failed_calculate = 0\n",
    "    for eachCode in tqdm(codes, desc=syntaxType):\n",
    "      try: \n",
    "        cleancode = eachCode.replace(\"<s> \", \"\").replace(\" </s>\", \"\").replace('<mask>', 'MASK')\n",
    "        types, rewrote_code = get_syntax_types_for_code(cleancode)\n",
    "        # send input to model\n",
    "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(rewrote_code))\n",
    "        input_ids = torch.tensor([tokenized_text])\n",
    "        # get reference indices\n",
    "        reference_indices = token_reference.generate_reference(input_ids.shape[1], device=device).unsqueeze(0)\n",
    "\n",
    "        layer_attrs = []\n",
    "        layer_attn_mat = []\n",
    "        input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, reference_indices)\n",
    "        # get layer attribution\n",
    "        for i in range(model.config.num_hidden_layers):\n",
    "          lc = LayerConductance(predict_forward_func, \n",
    "                              model.roberta.encoder.layer[i])\n",
    "          layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "                                                  baselines=ref_input_embeddings, \n",
    "                                                  additional_forward_args=())\n",
    "          layer_attrs.append(summarize_attributions(layer_attributions[0]))\n",
    "          layer_attn_mat.append(layer_attributions[1])\n",
    "          \n",
    "        # layer x seq_len\n",
    "        layer_attrs = torch.stack(layer_attrs)\n",
    "        # layer x batch x head x seq_len x seq_len\n",
    "        layer_attn_mat = torch.stack(layer_attn_mat)\n",
    "        # get start and end index of each token\n",
    "        start_end = get_start_end_of_token_when_tokenized(rewrote_code, types, tokenizer)\n",
    "        if syntaxType in types:\n",
    "          number += 1\n",
    "        for layer in range(12):\n",
    "          for head in range(12):\n",
    "            for each_sep_index in np.where(types==syntaxType)[0]:\n",
    "              start_index, end_index = start_end[each_sep_index]\n",
    "              interim_value = layer_attn_mat[layer][0][head][:, start_index:end_index+1].mean().cpu().detach().numpy()\n",
    "              if np.isnan(interim_value):\n",
    "                  pass\n",
    "              else: \n",
    "                  identifier[layer][head] += interim_value\n",
    "      except:\n",
    "        failed_calculate += 1\n",
    "    print(\"failed calculate: \", failed_calculate)\n",
    "                \n",
    "    identifier = identifier/number\n",
    "  return identifier, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_list = ['annotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "annotation: 100%|██████████| 6/6 [00:49<00:00,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed calculate:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "avg_attns = {}\n",
    "avg_attens_sum = {}\n",
    "syntax_frequenct = {}\n",
    "\n",
    "for syntax in syntax_list:\n",
    "    avg_attns[syntax] = np.zeros((12, 12))\n",
    "    avg_attns[syntax], syntax_frequenct[syntax] = getSyntaxAttributionScore(codes, tokenizer, syntax)\n",
    "    avg_attens_sum[syntax] = np.sum(avg_attns[syntax], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation': array([ 1.72173259e-06,  1.31034720e-06,  5.14218693e-07,  1.56241156e-06,\n",
       "        -1.41097455e-06,  1.09702434e-06, -1.45828861e-06,  4.52075069e-07,\n",
       "         5.28338201e-07,  4.70682549e-07, -1.07156853e-05,  5.47772742e-07])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_attens_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuBERT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8492d8711f92acbd6f1f5de70669e0e0dfa9ae8288673c2cd77f66771fab39c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
