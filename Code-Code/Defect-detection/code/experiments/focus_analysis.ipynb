{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')\n",
    "import torch\n",
    "from model import Model\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          BertConfig, BertForMaskedLM, BertTokenizer,\n",
    "                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_name_or_path = \"microsoft/codebert-base\"\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['roberta'] \n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_name_or_path, cache_dir=None, output_attentions=True)\n",
    "\n",
    "config.num_labels=1\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name_or_path,\n",
    "                                            do_lower_case=False,\n",
    "                                            cache_dir=None)\n",
    "block_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification(config)\n",
    "model=Model(model, config, tokenizer, None)\n",
    "# load checkpoints \n",
    "checkpoint_prefix = \"/home/fjiriges/CodeXGLUE/Code-Code/Defect-detection/code/saved_models/checkpoint-best-acc/model.bin\"\n",
    "model.load_state_dict(torch.load(checkpoint_prefix)) \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_tokens,\n",
    "                 input_ids,\n",
    "                 idx,\n",
    "                 label):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.idx=str(idx)\n",
    "        self.label=label\n",
    "        \n",
    "def convert_examples_to_features(js, tokenizer, block_size=512):\n",
    "    #source\n",
    "    code=' '.join(js['func'].split())\n",
    "    code_tokens=tokenizer.tokenize(code)[:block_size-2]\n",
    "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "    padding_length = block_size - len(source_ids)\n",
    "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = {'func': 'def min(a,b): if a > b: return a else: return b', 'idx': 0, 'target': 1}\n",
    "\n",
    "code_example = convert_examples_to_features(js, tokenizer, block_size=block_size)\n",
    "\n",
    "input_ids = torch.tensor([code_example.input_ids])\n",
    "input_labels = torch.tensor([code_example.label])\n",
    "input_tokens = code_example.input_tokens\n",
    "attention_mask=input_ids.ne(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "# generate reference indices for each sample\n",
    "token_reference = TokenReferenceBase(reference_token_idx=ref_token_id) # use padding_idx for roberta\n",
    "seq_length = len(code_example.input_ids)\n",
    "reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5386, grad_fn=<NegBackward0>) tensor([[0.5836]], grad_fn=<SigmoidBackward0>)\n",
      "0.5835870504379272 1\n"
     ]
    }
   ],
   "source": [
    "lm_loss, logit, output_attentions = model(input_ids=input_ids, \n",
    "                                          attention_mask=attention_mask,\n",
    "                                          labels=input_labels)\n",
    "pred = logit.item()\n",
    "pred_ind = round(pred)\n",
    "\n",
    "print(lm_loss, logit)\n",
    "print(pred, pred_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_ids=input_ids, attention_mask=attention_mask, labels=input_labels):\n",
    "    lm_loss, logit, output_attentions = model(input_ids=input_ids, \n",
    "                                          attention_mask=attention_mask,\n",
    "                                          labels=input_labels)\n",
    "    \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(predict, model.encoder.roberta.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute attributions and approximation delta using layer integrated gradients\n",
    "attributions_ig, delta = lig.attribute(input_ids, \n",
    "                                       reference_indices,\n",
    "                                       return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
    "attributions = attributions / torch.norm(attributions)\n",
    "attributions = attributions.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data_records_ig = []\n",
    "\n",
    "vis_data_records_ig.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            pred_ind,\n",
    "                            input_labels,\n",
    "                            \"Pos\",\n",
    "                            attributions.sum(),\n",
    "                            code_example.input_tokens,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualize attributions based on Integrated Gradients\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>tensor([1])</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.58)</b></text></td><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.62</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 55%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> def                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġmin                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> b                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ):                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġif                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġa                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġ>                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġb                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġreturn                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġa                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġelse                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġreturn                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ġb                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Visualize attributions based on Integrated Gradients')\n",
    "_ = visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuBERT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8492d8711f92acbd6f1f5de70669e0e0dfa9ae8288673c2cd77f66771fab39c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
