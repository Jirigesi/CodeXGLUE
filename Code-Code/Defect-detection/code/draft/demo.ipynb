{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients, LayerActivation\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'bert-base-uncased'\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path, output_attentions=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits, output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = model(inputs_embeds=inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 102, 101)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "ref_token_id, sep_token_id, cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "    \n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"What is important to us?\", \"It is important to us to include, empower and support humans of all kinds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = 'to include, empower and support humans of all kinds'\n",
    "\n",
    "ground_truth_tokens = tokenizer.encode(ground_truth, add_special_tokens=False)\n",
    "ground_truth_end_ind = indices.index(ground_truth_tokens[-1])\n",
    "ground_truth_start_ind = ground_truth_end_ind - len(ground_truth_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is important to us?\n",
      "Predicted Answer:  \n"
     ]
    }
   ],
   "source": [
    "start_scores, end_scores, output_attentions = predict(input_ids,\n",
    "                                   token_type_ids=token_type_ids, \\\n",
    "                                   position_ids=position_ids, \\\n",
    "                                   attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / norm_fn(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fjiriges/anaconda3/envs/cuBERT/lib/python3.8/site-packages/captum/attr/_models/base.py:188: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'bert.embeddings.word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_start = []\n",
    "layer_attrs_end = []\n",
    "\n",
    "layer_attn_mat_start = []\n",
    "layer_attn_mat_end = []\n",
    "\n",
    "input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=token_type_ids, ref_token_type_ids=ref_token_type_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LayerConductance(squad_pos_forward_func, model.bert.encoder.layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "                                  baselines=ref_input_embeddings, \n",
    "                                  additional_forward_args=(token_type_ids, position_ids,attention_mask, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.9876e-04, -2.7195e-04,  3.6985e-04,  ...,  5.3955e-04,\n",
       "           -4.0748e-05,  8.0837e-05],\n",
       "          [ 2.9033e-04,  1.2786e-04,  8.6897e-05,  ..., -1.1124e-04,\n",
       "            8.8540e-05,  3.6312e-04],\n",
       "          [ 5.8260e-04,  7.0691e-05,  5.3656e-05,  ...,  1.8078e-04,\n",
       "            1.6161e-04, -5.3487e-04],\n",
       "          ...,\n",
       "          [-2.6024e-05,  6.6708e-05, -3.9104e-05,  ...,  1.8256e-06,\n",
       "            1.4155e-04, -5.3608e-05],\n",
       "          [-7.0906e-05, -9.9923e-06, -7.0039e-05,  ..., -8.6263e-05,\n",
       "            4.8417e-05, -1.8701e-04],\n",
       "          [-6.6048e-05,  3.5066e-05, -8.3076e-06,  ...,  6.7862e-05,\n",
       "            2.0033e-05, -1.3639e-05]]], grad_fn=<SumBackward1>),\n",
       " tensor([[[[ 1.4943e-04,  1.1228e-05, -1.4513e-05,  ...,  8.1821e-06,\n",
       "             7.9674e-05,  1.9307e-04],\n",
       "           [ 1.6369e-05, -2.1236e-05,  4.2384e-05,  ...,  2.1004e-06,\n",
       "             3.1119e-06,  6.1411e-07],\n",
       "           [-1.4273e-04, -4.1959e-05,  2.7352e-05,  ..., -5.3862e-06,\n",
       "             4.5057e-06, -1.5606e-07],\n",
       "           ...,\n",
       "           [ 1.1176e-05,  9.2666e-08,  1.2897e-05,  ..., -6.3644e-06,\n",
       "             3.6546e-05, -3.5344e-06],\n",
       "           [ 3.8172e-05, -1.6188e-06, -1.0980e-06,  ..., -2.5955e-05,\n",
       "             9.0447e-06,  3.7833e-05],\n",
       "           [-2.2148e-05, -4.0440e-07,  2.9944e-07,  ...,  3.5921e-06,\n",
       "             2.5815e-05, -1.5001e-05]],\n",
       " \n",
       "          [[-2.3303e-03, -1.8622e-06, -8.3580e-06,  ...,  5.3994e-06,\n",
       "            -3.2018e-05, -7.2614e-06],\n",
       "           [ 2.7917e-05,  9.7955e-06,  1.7909e-06,  ..., -6.6138e-06,\n",
       "            -8.4110e-07,  2.0099e-06],\n",
       "           [-1.2683e-04,  2.1102e-05, -3.2078e-05,  ..., -6.4558e-06,\n",
       "             7.0841e-06,  2.1064e-06],\n",
       "           ...,\n",
       "           [-7.0789e-07,  2.3966e-06,  3.0469e-06,  ...,  2.5487e-06,\n",
       "            -2.7832e-05, -8.3273e-06],\n",
       "           [ 8.7080e-07,  2.7759e-07, -4.7241e-06,  ..., -1.1157e-05,\n",
       "             5.7939e-05, -4.7660e-06],\n",
       "           [-6.8350e-05,  1.4085e-06,  1.4947e-06,  ...,  4.9139e-05,\n",
       "            -6.4871e-06, -1.0814e-04]],\n",
       " \n",
       "          [[-1.4208e-04,  2.9771e-06,  1.2612e-04,  ...,  1.7558e-05,\n",
       "             1.0424e-04, -1.4772e-05],\n",
       "           [ 1.0278e-05, -4.6327e-05,  2.7168e-07,  ...,  8.9802e-07,\n",
       "            -3.7565e-06,  1.3202e-05],\n",
       "           [ 8.0510e-05, -4.4183e-04, -8.7430e-06,  ...,  4.3340e-07,\n",
       "            -5.8263e-07, -6.0304e-06],\n",
       "           ...,\n",
       "           [-3.6382e-06, -1.0315e-05, -1.0706e-06,  ..., -7.5623e-06,\n",
       "            -1.6682e-05, -4.7837e-06],\n",
       "           [ 2.2026e-04, -3.4046e-07, -4.5651e-06,  ..., -5.6218e-04,\n",
       "            -6.5472e-05,  3.0899e-05],\n",
       "           [ 7.2751e-05,  3.5671e-07, -5.0764e-07,  ...,  3.6934e-05,\n",
       "            -9.7610e-04, -2.4710e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.1587e-04, -3.2862e-04,  1.9544e-04,  ..., -1.3581e-04,\n",
       "             1.7142e-04, -1.0295e-04],\n",
       "           [ 2.0715e-04, -1.0586e-05,  1.4295e-05,  ..., -4.4998e-05,\n",
       "            -6.3197e-05, -1.2186e-06],\n",
       "           [-9.4086e-05, -4.6043e-05, -3.2619e-05,  ..., -1.1829e-05,\n",
       "            -7.2331e-06, -1.7658e-04],\n",
       "           ...,\n",
       "           [ 1.9679e-05, -5.0248e-05,  1.9599e-05,  ...,  2.3464e-04,\n",
       "            -1.7547e-05, -2.1787e-04],\n",
       "           [-1.1910e-04, -1.3123e-05, -2.0458e-05,  ...,  2.6305e-05,\n",
       "            -2.9692e-06,  2.1052e-05],\n",
       "           [ 5.2929e-05, -6.2168e-06, -3.9877e-07,  ...,  5.1567e-07,\n",
       "             3.7890e-08,  8.8090e-06]],\n",
       " \n",
       "          [[-1.3852e-05,  1.2596e-04, -1.2549e-05,  ..., -1.1122e-05,\n",
       "             1.0326e-05,  1.8969e-07],\n",
       "           [ 1.1150e-03,  1.8888e-05,  2.7613e-04,  ...,  1.4313e-06,\n",
       "            -3.6130e-07,  1.7578e-06],\n",
       "           [ 6.4608e-04,  4.2623e-05,  1.0737e-04,  ..., -5.0972e-08,\n",
       "            -5.1609e-08, -5.8568e-08],\n",
       "           ...,\n",
       "           [ 7.9174e-04, -8.4834e-07, -1.7729e-07,  ..., -4.5296e-06,\n",
       "             5.6428e-04, -7.1545e-05],\n",
       "           [ 2.3127e-04, -1.0692e-07, -7.1785e-08,  ...,  1.8643e-05,\n",
       "             2.0054e-05, -5.6931e-04],\n",
       "           [-1.3662e-05, -2.4849e-06, -1.0979e-06,  ...,  9.5771e-06,\n",
       "             5.4965e-05, -8.3258e-05]],\n",
       " \n",
       "          [[-1.7117e-04,  4.7463e-06,  1.9612e-06,  ..., -2.3168e-05,\n",
       "            -1.5249e-05, -6.0900e-07],\n",
       "           [-4.6395e-05, -4.0246e-05, -4.3748e-06,  ..., -4.8401e-07,\n",
       "            -2.9883e-05, -9.8307e-07],\n",
       "           [-3.7679e-04, -3.3048e-04,  2.7360e-05,  ..., -9.2835e-06,\n",
       "             1.2368e-06, -9.3921e-06],\n",
       "           ...,\n",
       "           [-7.5468e-05,  1.0176e-05, -3.7156e-07,  ..., -6.2409e-05,\n",
       "            -6.1776e-05, -3.8757e-05],\n",
       "           [ 1.4236e-06, -2.9091e-06, -1.8804e-06,  ...,  5.0446e-05,\n",
       "            -4.2431e-05, -1.5885e-05],\n",
       "           [ 6.0057e-06,  8.3693e-07, -4.8785e-07,  ..., -2.6169e-05,\n",
       "             1.0023e-05,  8.6170e-06]]]], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuBERT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8492d8711f92acbd6f1f5de70669e0e0dfa9ae8288673c2cd77f66771fab39c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
